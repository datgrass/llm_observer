{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b69be5b9-144a-4e19-857e-0c044a94837b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Anthropic Stress Test Demo\n",
    "-------------\n",
    "\n",
    "### Purpose\n",
    "- Create randomized datasets of various sizes and test different LLM's performance against them.\n",
    "  - Standardize / track input and output tokens\n",
    "  - Summarize each row with AI query\n",
    "  - Capture a **rough estimate** of DBU consumption, cost, and total Runtime\n",
    "    - *Actual costs may differ from what this process indicates*\n",
    "\n",
    "----------------------\n",
    "### Links:\n",
    "- https://www.databricks.com/product/pricing/proprietary-foundation-model-serving\n",
    "- https://docs.databricks.com/aws/en/large-language-models/ai-functions#-general-purpose-function-ai_query\n",
    "\n",
    "-----------------------\n",
    "\n",
    "### Input Parameters\n",
    "- **Catalog**\n",
    "  - The desired catalog for results to be written to upon completion.\n",
    "- **Schema**\n",
    "  - The desired schema within the output catalog for results to be written.\n",
    "- **Model Endpoints to Run**\n",
    "  - A comma seperated list of endpoints available to you in databricks.\n",
    "    - e.g. \"databricks-claude-sonnet-4-5, databricks-claude-opus-4-5, databricks-claude-opus-4-1\"\n",
    "      - ***exclude the quotes when inputing data***\n",
    "- **Sample Dataset Sizes**\n",
    "  - A comma seperated list of numbers to determine how large our sample datasets will be. Tables smaller than 100 rows will be filtered out from logging results.\n",
    "    - e.g. \"100, 1000, 10000\"\n",
    "      - ***exclude the quotes when inputing data***\n",
    "- **Time Zone**\n",
    "  - The time zone of the region your workspace is in. For example, if your workspace is in AWS us-east-2, set your time zone to something like America/New_York\n",
    "\n",
    "------------------\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6bbd127d-0533-4947-bf1b-7c47d12b080f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from pyspark.sql import Window\n",
    "from pyspark.sql.types import StructType, StructField, StringType, TimestampType, IntegerType, DoubleType\n",
    "from stress_test import create_widgets, generate_sales_data, run_sample_query, to_valid_table_name\n",
    "\n",
    "create_widgets(dbutils, '02_llm_performance_test')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61b2c958-a290-49af-b563-2e01f2fc9d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Defining Helper Functions\n",
    "- **generate_sales_data**\n",
    "  - Creates a randomized dataset of a specified size with the following features:\n",
    "    - *date*: the date a transaction occurred\n",
    "    - *average_temperature*: the temp in celsius for the day of the transaction\n",
    "    - *rainfall*: the rainfall in cm for the day of the transaction\n",
    "    - *weekend*: if the transaction was on a weekend\n",
    "    - *holiday*: if the transaction was on a holiday\n",
    "    - *price_per_kg*: the price per kg of goods for the transaction\n",
    "    - *demand*: the amount of goods purchased in kgs\n",
    "    - *month*: the month number of the transaction\n",
    "    - *total_spend*: the total amount spent on the transaction\n",
    "\n",
    "- **run_sample_query**\n",
    "  - Executes a given sql query and prints the amount of time to completion. Also can optionally write the output of the query to a specified location if provided.\n",
    "\n",
    "- **to_valid_table_name**\n",
    "  - Converts a given string into a valid databricks table name by performing a series of regex manipulations."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b1bffb8b-d16b-491b-a524-acb84ada4ab4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----------------\n",
    "### Get the values from our widgets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a67498a3-a674-4950-93e6-5dd84ea0cdeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Set the sizes for our randomized datasets\n",
    "record_counts = [int(num.strip()) for num in dbutils.widgets.get('sample_dataset_sizes').split(',')]\n",
    "\n",
    "# Set the endpoints for the models we want to use\n",
    "model_endpoints = [model.strip() for model in dbutils.widgets.get('model_endpoints').split(',')]\n",
    "\n",
    "# Set the catalog and schema for the outputs\n",
    "catalog = dbutils.widgets.get('catalog')\n",
    "schema = dbutils.widgets.get('schema')\n",
    "\n",
    "# get timezone for spark jobs\n",
    "time_zone = dbutils.widgets.get('time_zone')\n",
    "spark.conf.set('spark.sql.session.timeZone', time_zone)\n",
    "\n",
    "# The prompt we want to give our ai_query\n",
    "prompt = \"\"\"\n",
    "You will be given a dataset with the following fields:\n",
    "- date: the date a transaction occurred\n",
    "- average_temperature: the temp in celsius for the day of the transaction\n",
    "- rainfall: the rainfall in cm for the day of the transaction\n",
    "- weekend: if the transaction was on a weekend\n",
    "- holiday: if the transaction was on a holiday\n",
    "- price_per_kg: the price per kg of goods for the transaction\n",
    "- demand: the amount of goods purchased in kgs\n",
    "- month: the month number of the transaction\n",
    "- total_spend: the total amount spent on the transaction\n",
    "\n",
    "Provide me with concise observations (2 sentences max) about the following transactions:\n",
    "\"\"\"\n",
    "\n",
    "# SQL to execute, leave space open for model and prompt\n",
    "sql = \"\"\"\n",
    "        SELECT\n",
    "            *,\n",
    "            ai_query(\n",
    "                '{model}',\n",
    "                '{prompt}' ||\n",
    "                concat_ws(', ',\n",
    "                    CAST(date AS STRING),\n",
    "                    CAST(average_temperature AS STRING),\n",
    "                    CAST(rainfall AS STRING),\n",
    "                    CAST(weekend AS STRING),\n",
    "                    CAST(holiday AS STRING),\n",
    "                    CAST(price_per_kg AS STRING),\n",
    "                    CAST(demand AS STRING),\n",
    "                    CAST(month AS STRING),\n",
    "                    CAST(total_spend AS STRING)\n",
    "                )\n",
    "            ) AS summary\n",
    "        FROM sample_records\n",
    "    \"\"\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ec632a07-94f2-4997-9c38-d937993634dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----------------------\n",
    "\n",
    "## Executing the Queries\n",
    "\n",
    "- The process:\n",
    "  1. Loop through each data set size from ***record_counts***\n",
    "      - Generate a dataset with given number of rows and create a temporary view to be used in the sql query\n",
    "  2. Loop through each model in ***model_endpoints***\n",
    "      - For every dataset size we will run each model specified to compare results later.\n",
    "  3. Write query results\n",
    "      - Save results from queries for future reference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77282e1a-0885-49c1-983e-1f1fdde42d63",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create an empty list to store query information\n",
    "run_logs = []\n",
    "\n",
    "# Loop through each data set size\n",
    "for record_count in record_counts:\n",
    "    print(f'Generating {record_count} records...\\n')\n",
    "    sample_records = generate_sales_data(record_count)\n",
    "    spark.createDataFrame(sample_records) \\\n",
    "        .createOrReplaceTempView('sample_records')\n",
    "    \n",
    "    # Query the dataset with each model\n",
    "    for model in model_endpoints:\n",
    "        print(f'Running model: {model}')\n",
    "        run_name = to_valid_table_name(model)\n",
    "        table_name = f'{run_name}_results_{record_count}'\n",
    "        run_sql = sql.format(model=model, prompt=prompt)\n",
    "        \n",
    "        # Attempt to execute the query and log the results\n",
    "        try:\n",
    "            run_log = run_sample_query(spark, run_sql, catalog, schema, table_name)\n",
    "            run_log['model'] = model\n",
    "            run_log['run_name'] = run_name\n",
    "            \n",
    "            run_logs.append(run_log)\n",
    "            print('Elapsed time:', run_log['elapsed_time'])\n",
    "            print('-' * 80, '\\n')\n",
    "        except Exception as e:\n",
    "            print(e)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "09bf7dca-b1ce-4586-a349-5b16fe8f25f0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Log our runs\n",
    "logging_schema = StructType([\n",
    "    StructField('run_id', StringType(), True),\n",
    "    StructField('run_name', StringType(), True),\n",
    "    StructField('model', StringType(), True),\n",
    "    StructField('output_table', StringType(), True),\n",
    "    StructField('n_rows', IntegerType(), True),\n",
    "    StructField('n_columns', IntegerType(), True),\n",
    "    StructField('start_time', TimestampType(), True),\n",
    "    StructField('end_time', TimestampType(), True),\n",
    "    StructField('elapsed_time', DoubleType(), True),\n",
    "    StructField('sql', StringType(), True),\n",
    "])\n",
    "\n",
    "logging_df = spark.createDataFrame(run_logs, schema=logging_schema)\n",
    "logging_df = logging_df.withColumn('insert_time', f.current_timestamp())\n",
    "logging_df.write.mode('append').option('mergeSchema','true').saveAsTable(f'{catalog}.{schema}.runs')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ae738156-9be7-44ac-a95f-42f1f54e38b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-------------------\n",
    "\n",
    "## View the Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ccbf932b-e650-4ed0-b21e-3f310d309208",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "SELECT * FROM identifier(:catalog || '.' || :schema || '.runs')\n",
    "ORDER BY start_time DESC"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "42963cbd-336b-4c8e-85f8-5e6112e6a1dd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "--------------\n",
    "\n",
    "## Analyze the Queries with Databricks System Tables\n",
    "\n",
    "- We need to track our query performance and understand how each model performs on identical datasets.\n",
    "\n",
    "- The first step is to find our queries in the **system.queries.history** table .\n",
    "  - This will allow us to identify start / end times for our queries and analyze run times."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6cbb99ee-a7b2-4a3b-9dbf-7d3802bd8014",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW model_queries AS\n",
    "SELECT \n",
    "  account_id,\n",
    "  workspace_id,\n",
    "  statement_id,\n",
    "  executed_by,\n",
    "  total_duration_ms,\n",
    "  execution_duration_ms,\n",
    "  compilation_duration_ms,\n",
    "  result_fetch_duration_ms,\n",
    "  start_time,\n",
    "  end_time\n",
    "FROM system.query.history\n",
    "WHERE\n",
    "  statement_text = \"llm_observer_sdf.write.mode('overwrite').saveAsTable(f'{llm_observer_catalog}.{llm_observer_schema}.{llm_observer_table_name}')\"\n",
    "ORDER BY\n",
    "  start_time DESC\n",
    ";\n",
    "\n",
    "SELECT * FROM model_queries"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "abd9d609-c8c2-4f61-a6fc-dc7486ec040c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "- Now we can join the data from the 2 views together. We still don't have a concrete id to map them together, but we can link them together based on their start times.\n",
    "\n",
    "- The query starts just seconds before the first model run starts. Because these models take so long to complete, we can assume that if there is a query start and a model run within 5 seconds of each other, those are the same record."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4512233f-900f-4773-b3ac-56717853b427",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Construct the table name in Python\n",
    "table_name = f\"{catalog}.{schema}.runs\"\n",
    "\n",
    "# Use the constructed table name in the SQL query\n",
    "query = f\"\"\"\n",
    "CREATE OR REPLACE TEMPORARY VIEW run_query_mapping AS\n",
    "SELECT\n",
    "  t2.executed_by,\n",
    "  t1.run_id,\n",
    "  t1.output_table,\n",
    "  t1.n_rows,\n",
    "  t1.start_time as run_start_time,\n",
    "  t1.end_time as run_end_time,\n",
    "  t2.start_time AS query_start_time,\n",
    "  t2.end_time AS query_end_time,\n",
    "  t2.total_duration_ms / 60000 AS total_duration_minutes,\n",
    "  t2.statement_id\n",
    "FROM {table_name} t1\n",
    "INNER JOIN model_queries t2 ON\n",
    "  abs(unix_timestamp(t1.start_time) - unix_timestamp(t2.start_time)) <= 5\n",
    "WHERE \n",
    "  t1.n_rows >= 100\n",
    "ORDER BY\n",
    "  t1.start_time DESC\n",
    "\"\"\"\n",
    "\n",
    "# Run the query\n",
    "spark.sql(query)\n",
    "\n",
    "# Display the results\n",
    "display(spark.sql(\"SELECT * FROM run_query_mapping\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d8ecf90-fddf-4ef6-8368-b5a26bfd19a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-------------\n",
    "### Get data about the batch processing calls"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4f6b7486-2927-477a-b275-0e26c69b2ff9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TEMPORARY VIEW token_count AS\n",
    "SELECT \n",
    "  t1.account_id,\n",
    "  t1.workspace_id,\n",
    "  t1.served_entity_id,\n",
    "  t2.endpoint_name,\n",
    "  t1.requester,\n",
    "  t1.request_time,\n",
    "  t1.input_token_count,\n",
    "  t1.output_token_count,\n",
    "  t1.input_character_count,\n",
    "  t1.output_character_count\n",
    "FROM system.serving.endpoint_usage t1\n",
    "INNER JOIN system.serving.served_entities t2 ON\n",
    "  t1.served_entity_id = t2.served_entity_id\n",
    ";\n",
    "\n",
    "SELECT * FROM token_count"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4912d358-bb34-465b-ab54-37614d13f916",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "------------\n",
    "\n",
    "## Get cost data and filter for results\n",
    "- Models may have multiple cost configurations depending on usage, region, etc.\n",
    "  - The system tables don't give us enough information about region and context window so **we make assumptions**\n",
    "- This process gets the first available cost configuration for a model\n",
    "  - ***This cost configuration may not be 100% accurate to your given use case***\n",
    "  - Regardless, this cost estimate should be close to the actual values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9d7032a1-e713-4972-a444-a4aab661a757",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1765981104549}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get data for natively served model cost\n",
    "cost_df = spark.sql(f'select * from {catalog}.{schema}.llm_cost_mapping')\n",
    "\n",
    "# window over model name\n",
    "window = Window.partitionBy('model').orderBy(['model','endpoint_type','context_length'])\n",
    "\n",
    "# get row count by model name and filter for the first record returned\n",
    "cost_df = cost_df.withColumn('row_number', f.row_number().over(window))\n",
    "cost_df = cost_df.filter(f.col('row_number')==1)\n",
    "cost_df.createOrReplaceTempView('cost_df')\n",
    "display(cost_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fb9ee5a-d346-47a3-bfef-4e3f61bbc543",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "-----------\n",
    "\n",
    "## Save final table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0ad3ce97-7e4e-416b-b29e-c8b42b74bd4b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE OR REPLACE TABLE identifier(:catalog || '.' || :schema || '.model_testing_results') AS\n",
    "SELECT \n",
    "  t5.*,\n",
    "  t5.input_dbus + t5.output_dbus as total_dbus,\n",
    "  t5.input_dbus * 0.07 as input_dbu_cost,\n",
    "  t5.output_dbus * 0.07 as output_dbu_cost,\n",
    "  (t5.input_dbus + t5.output_dbus) * 0.07 as total_dbu_cost\n",
    "FROM (\n",
    "  SELECT \n",
    "    t4.*,\n",
    "    t4.input_token_count / 1000000 * t4.input_dbu_1m_tokens as input_dbus,\n",
    "    t4.output_token_count / 1000000 * t4.output_dbu_1m_tokens as output_dbus\n",
    "  FROM (\n",
    "    SELECT \n",
    "      t1.*,\n",
    "      t2.requester,\n",
    "      t2.request_time,\n",
    "      t2.served_entity_id,\n",
    "      t2.endpoint_name,\n",
    "      t2.input_token_count,\n",
    "      t2.output_token_count,\n",
    "      t2.input_token_count + t2.output_token_count AS total_tokens,\n",
    "      t2.input_character_count,\n",
    "      t2.output_character_count,\n",
    "      t2.input_character_count + t2.output_character_count AS total_characters,\n",
    "      t3.input_dbu_1m_tokens,\n",
    "      t3.output_dbu_1m_tokens\n",
    "    FROM run_query_mapping t1\n",
    "    INNER JOIN token_count t2 ON\n",
    "      t2.request_time > t1.query_start_time \n",
    "      AND t2.request_time < t1.query_end_time\n",
    "      AND t1.executed_by = t2.requester\n",
    "    INNER JOIN cost_df t3 ON\n",
    "      t2.endpoint_name = t3.served_endpoint_name\n",
    "  ) t4\n",
    ") t5\n",
    "WHERE\n",
    "  t5.input_dbu_1m_tokens IS NOT NULL\n",
    ";\n",
    "\n",
    "SELECT * FROM identifier(:catalog || '.' || :schema || '.model_testing_results')"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [
    {
     "elements": [],
     "globalVars": {},
     "guid": "",
     "layoutOption": {
      "grid": true,
      "stack": true
     },
     "nuid": "09d801c9-07be-4a58-8158-80fbad445032",
     "origId": 5316284587039615,
     "title": "Untitled",
     "version": "DashboardViewV1",
     "width": 1024
    }
   ],
   "environmentMetadata": null,
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 5316284587039614,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 2
   },
   "notebookName": "02_llm_performance_test",
   "widgets": {
    "catalog": {
     "currentValue": "llm_observer",
     "nuid": "a191f961-24b6-4b15-8861-c28f3b2e391b",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "llm_observer",
      "label": "01. Catalog",
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "llm_observer",
      "label": "01. Catalog",
      "name": "catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "model_endpoints": {
     "currentValue": "databricks-claude-sonnet-4-5, databricks-claude-opus-4-5, databricks-claude-opus-4-1",
     "nuid": "61d64fd8-b1ff-4a0d-bed1-1eb48f123042",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-claude-sonnet-4-5, databricks-claude-opus-4-5",
      "label": "03. Model Endpoints",
      "name": "model_endpoints",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks-claude-sonnet-4-5, databricks-claude-opus-4-5",
      "label": "03. Model Endpoints",
      "name": "model_endpoints",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "sample_dataset_sizes": {
     "currentValue": "100, 1000",
     "nuid": "d533b4ed-185b-4534-a68f-232981f1aef5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "100,200",
      "label": "04. Sample Dataset Sizes",
      "name": "sample_dataset_sizes",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "100,200",
      "label": "04. Sample Dataset Sizes",
      "name": "sample_dataset_sizes",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema": {
     "currentValue": "default",
     "nuid": "994ce368-dbf9-42c9-b0ed-c3bbd805b158",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "02. Schema",
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "default",
      "label": "02. Schema",
      "name": "schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "time_zone": {
     "currentValue": "America/Detroit",
     "nuid": "a79a37a0-4fda-4607-a96b-23b20d12e0e5",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "America/Detroit",
      "label": "05. Time Zone",
      "name": "time_zone",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "America/Detroit",
      "label": "05. Time Zone",
      "name": "time_zone",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
