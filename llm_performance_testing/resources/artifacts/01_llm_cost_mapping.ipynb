{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "55e9c7c4-ff23-4840-b62d-8d774e8e1dee",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "## Anthropic Stress Test Demo\n",
    "- Task: `01_llm_cost_mapping`\n",
    "-------------\n",
    "\n",
    "### Purpose\n",
    "- Combine the separate web page datasets into a single table.\n",
    "- Use AI to map the model names to the databricks natively served models.\n",
    "\n",
    "-----------------------\n",
    "\n",
    "### Input Parameters\n",
    "- **Catalog**\n",
    "  - The desired catalog for results to be written to upon completion.\n",
    "- **Schema**\n",
    "  - The desired schema within the output catalog for results to be written.\n",
    "- **Mapping Model**\n",
    "  - The model endpoint that will be used in `ai_query` to figure out the relevant endpoint for each model in the pricing dataset.\n",
    "    - Currently set by default to `databricks-meta-llama-3-3-70b-instruct`\n",
    "\n",
    "\n",
    "-----------------------\n",
    "\n",
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3e74fbf3-23ac-49ca-93f0-83e73738c335",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import functions as f\n",
    "from stress_test import create_widgets\n",
    "\n",
    "create_widgets(dbutils, '01_llm_cost_mapping')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "0cb68f06-e2a4-4b30-82f0-240632a66486",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "--------\n",
    "## Get data from tables  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "425e3f80-cded-4521-b643-a0f2f82e754e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get catalog and schema\n",
    "catalog = dbutils.widgets.get('catalog')\n",
    "schema = dbutils.widgets.get('schema')\n",
    "mapping_model = dbutils.widgets.get('mapping_model_endpoint')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c5fbdc51-5c87-4444-949a-496651a46ce0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "oss_df = spark.sql(f'select * from {catalog}.{schema}.llm_oss_costs')\n",
    "display(oss_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c650cc31-30e7-45fa-9b99-d13b67f19f1b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "prop_df = spark.sql(f'select * from {catalog}.{schema}.llm_proprietary_costs')\n",
    "display(prop_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9aa33549-92c7-4828-9836-d31841fe98a0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "--------\n",
    "## Union datasets together and fill null values"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6df133e6-3db0-4bcf-b92e-b0f5254d5d00",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# union datasets together and allow for missing columns between datasets\n",
    "union_df = oss_df.unionByName(prop_df, allowMissingColumns = True)\n",
    "\n",
    "# fill nulls for endpoint_type and context_length\n",
    "union_df = union_df.fillna('Global', subset='endpoint_type')\n",
    "union_df = union_df.fillna('All Lengths', subset='context_length')\n",
    "union_df.createOrReplaceTempView('union_df')\n",
    "display(union_df)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8447044c-a283-4ac2-a24f-820373ac02b3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---------------\n",
    "\n",
    "## Get list of databricks natively served model endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c4350c2-2172-401f-b5bf-a60e1e2d6834",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# get distinct list of model endpoint names\n",
    "served_models = spark.sql('select distinct endpoint_name from system.serving.served_entities')\n",
    "\n",
    "# filter for endpoints that start with \"databricks\" (natively served models)\n",
    "served_models = served_models.filter(f.col('endpoint_name').like('databricks%'))\n",
    "served_models = served_models.toPandas()['endpoint_name'].to_list()\n",
    "\n",
    "# concat into a single string to pass to a prompt\n",
    "served_models = ', '.join(served_models)\n",
    "print(served_models)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "8cf86b06-d0c3-4b9c-ab18-c2bb11ae61da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "---------\n",
    "## Use AI to determine relevant model endpoints"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b545f30f-5554-4420-839c-b7bddffd83c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# define the prompt that we will pass along to ai_query\n",
    "prompt = f\"\"\"\n",
    "You are an assistant designed to map the LLM model name to the served model endpoint provided. You will be given a spark dataframe of model names and a comma seperated list of served model endpoints. You will return the served endpoint most likely associated with the model name. If there is no clear match, return \"none\". Please do not include a summary or any additional text, return only the mapped model name. Thank you!\n",
    "\n",
    "Example:\n",
    "  - model: GPT 5.1\n",
    "  - output: databricks-gpt-5-1\n",
    "Example:\n",
    "  - model: Mistral 7B\n",
    "  - output: none\n",
    "\n",
    "served_models: {served_models}\n",
    "\"\"\"\n",
    "\n",
    "# define the sql query that we will run\n",
    "sql = f\"\"\"\n",
    "SELECT\n",
    "  model,\n",
    "  ai_query('{mapping_model}','{prompt}' || model) AS served_endpoint_name,\n",
    "  provider,\n",
    "  endpoint_type,\n",
    "  context_length,\n",
    "  input_dbu_1m_tokens,\n",
    "  output_dbu_1m_tokens,\n",
    "  cache_writes_dbu_1m_tokens,\n",
    "  cache_reads_dbu_1m_tokens,\n",
    "  batch_inference_dbu_hour,\n",
    "  dbu_per_hour_entry_cap,\n",
    "  dbu_per_hour_scale_cap\n",
    "FROM\n",
    "  union_df\n",
    "\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d9576ceb-b922-4801-8b56-3ba047779bbd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# execute the query and save as a table\n",
    "mapped_df = spark.sql(sql)\n",
    "mapped_df.write.mode('overwrite').saveAsTable(f'{catalog}.{schema}.llm_cost_mapping')\n",
    "display(mapped_df)"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "01_llm_cost_mapping",
   "widgets": {
    "catalog": {
     "currentValue": "llm_observer",
     "nuid": "bc2ef367-3fcf-4b02-84e6-5389aead465d",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "llm_observer",
      "label": "01. Catalog",
      "name": "catalog",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "llm_observer",
      "label": "01. Catalog",
      "name": "catalog",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "mapping_model_endpoint": {
     "currentValue": "databricks-meta-llama-3-3-70b-instruct",
     "nuid": "7b13d098-0142-4458-92a1-8d4b32401f60",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "databricks-meta-llama-3-3-70b-instruct",
      "label": "03. Mapping Model",
      "name": "mapping_model_endpoint",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "databricks-meta-llama-3-3-70b-instruct",
      "label": "03. Mapping Model",
      "name": "mapping_model_endpoint",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    },
    "schema": {
     "currentValue": "default",
     "nuid": "7c22643b-b1be-47b7-a68b-c6eaaa2fdab1",
     "typedWidgetInfo": {
      "autoCreated": false,
      "defaultValue": "default",
      "label": "02. Schema",
      "name": "schema",
      "options": {
       "validationRegex": null,
       "widgetDisplayType": "Text"
      },
      "parameterDataType": "String"
     },
     "widgetInfo": {
      "defaultValue": "default",
      "label": "02. Schema",
      "name": "schema",
      "options": {
       "autoCreated": null,
       "validationRegex": null,
       "widgetType": "text"
      },
      "widgetType": "text"
     }
    }
   }
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
